{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c2fb22-0e16-42f9-9d8a-d7623e374a82",
   "metadata": {},
   "source": [
    "# Language model: Supervised Fine-Tune (STF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d2de0-4a2b-44b4-894d-5f51c47675c5",
   "metadata": {},
   "source": [
    "Code adapted from LiquidAI's Supervised Fine-Tuning (SFT) notebook with a LoRA adapter in TRL (https://colab.research.google.com/drive/1j5Hk_SyBb2soUsuhU0eIEA9GwLNRnElF?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603f019-a5c8-4d94-a739-148451228a9f",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6277ce-2511-4367-befc-14ee98892d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers.git trl>=0.18.2 peft>=0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27796364-509e-42da-bd43-11add7007a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "norm_ranges = pd.read_csv('norm_range.csv', sep=';', names=['feature', 'lower', 'upper'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f6356-f83c-43db-b617-6bf5775aef4f",
   "metadata": {},
   "source": [
    "## Fine-tuning dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6fad89-f850-4962-bfb3-f696fcc4cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    " You are a clinician. The user will provide a dataset in JSON format\n",
    " containing a collection of supporting features and counterpoint features.\n",
    " For each feature, there is the value and its expected range.\n",
    " The text must be based only on the provided data.\"\"\".replace(\"\\n \", \" \")\n",
    "\n",
    "def get_user_prompt(explanations_df, prediction, model):\n",
    "    # The task and output format definition is the same for all user prompts\n",
    "    start = \"\"\"\n",
    "     Summarise the patient data into 3 cohesive and medical term-rich paragraphs\n",
    "     of 100 words each, without bullet points. First, the diagnosis\n",
    "     support (use only features from the Support section). Second, the\n",
    "     diagnosis counterpoints (use only features from the Counterpoints\n",
    "     section). Third, categorise the given diagnosis into possible\n",
    "     or impossible based on the data and justify.\"\"\".replace(\"\\n     \", \" \")\n",
    "\n",
    "    # Gather patient data \n",
    "    df = explanations_df[['feature', 'name', 'value', f'importance-{model}']]\n",
    "    df = df.rename(columns={f'importance-{model}': 'importance'})\n",
    "    df['value'] = [float(f'{val:.2f}') for val in df['value']]\n",
    "    df['expected range'] = [f\"[{norm_ranges[norm_ranges['feature'] == col]['lower'].iloc[0]}, \\\n",
    "{norm_ranges[norm_ranges['feature'] == col]['upper'].iloc[0]}]\" \\\n",
    "                   for col in df['feature']]\n",
    "    \n",
    "    # Support data\n",
    "    df_p = df[df['importance'] > 0.01]\n",
    "    if len(df_p) == 0:\n",
    "        p_prompt = 'No significative supporting data'\n",
    "    else:\n",
    "        df_p = df_p.sort_values(by='importance', ascending=False)[:7]\n",
    "        p_prompt = df_p[['name', 'value', 'expected range']].T.to_dict()\n",
    "        p_prompt = [str(val).replace(\"'\", \"\").replace('\"', \"\") for val in p_prompt.values()]\n",
    "    \n",
    "    # Counterpoint data\n",
    "    df_n = df[df['importance'] < -0.01]\n",
    "    if len(df_n) == 0:\n",
    "        n_prompt = 'No significative counterpoints'\n",
    "    else:\n",
    "        df_n = df_n.sort_values(by='importance', ascending=True)[:7]\n",
    "        n_prompt = df_n[['name', 'value', 'expected range']].T.to_dict()\n",
    "        n_prompt = [str(val).replace(\"'\", \"\").replace('\"', \"\") for val in n_prompt.values()]\n",
    "    \n",
    "    # Join data\n",
    "    prompt = f'{start}\\n\\nSupport: {p_prompt}\\nCounterpoints: {n_prompt}\\nPrediction: {prediction}'\n",
    "    return prompt\n",
    "\n",
    "# Construct final message\n",
    "def get_messages(explanations_df, prediction, model):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": get_user_prompt(explanations_df, prediction, model)}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ae4ef2-6e88-4c50-91c9-1f3efabdbd83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load explanations from the model\n",
    "patients_id = os.listdir('results')\n",
    "model_name = [\"xgboost\", \"rf\"]\n",
    "cat_map = {0: 'Cognitive Normal', 1: 'Mild Cognitive Impairment', 2: \"Alzheimer's Disease\"} \n",
    "\n",
    "messages = list()\n",
    "for ptid in patients_id:\n",
    "    path = f\"results/{ptid}/\"\n",
    "    expl_cn = pd.read_csv(path + f\"explanations-cn.csv\")\n",
    "    expl_mci = pd.read_csv(path + f\"explanations-mci.csv\")\n",
    "    expl_ad = pd.read_csv(path + f\"explanations-ad.csv\")\n",
    "    expls = [expl_cn, expl_mci, expl_ad]\n",
    "    probs = dict()\n",
    "    for model in model_name:\n",
    "        for i, cat in cat_map.items():\n",
    "            messages.append(get_messages(expls[i], cat, model))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ee3460-fb42-47b6-bc07-758908f5f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('messages.pkl', 'wb') as f:\n",
    "    pickle.dump(messages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece388da-19d5-4878-8da2-6093ee760dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gemini model\n",
    "GEMINI_API_KEY = \"\"\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9307587-6190-4a67-927d-61cce99bb58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603\n"
     ]
    }
   ],
   "source": [
    "# Read progress to continue with the generation\n",
    "sep = '\\u241E'  # Special character to separate lines safely\n",
    "try:\n",
    "    with open('responses.csv', 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        responses_done = content.split(sep)\n",
    "except FileNotFoundError:\n",
    "    responses_done = list()\n",
    "print(len(responses_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3df8590-281a-47ac-8f9c-ef4ab5c72b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619\n",
      "635\n",
      "665\n",
      "681\n",
      "713\n",
      "744\n",
      "776\n",
      "804\n",
      "Daily limit met\n"
     ]
    }
   ],
   "source": [
    "# Generate responses (adjusted for rate limits)\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    current = len(responses_done)\n",
    "    try:\n",
    "        responses = list()\n",
    "        \n",
    "        for i, msg in enumerate(messages[current:]):\n",
    "            sys_prompt = msg[0]['content']\n",
    "            user_prompt = msg[1]['content']\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\", \n",
    "                config=types.GenerateContentConfig(\n",
    "                    system_instruction=sys_prompt),\n",
    "                contents=user_prompt,\n",
    "            )\n",
    "            responses.append(response.text)\n",
    "    except:\n",
    "        if len(responses) == 0:\n",
    "            print('Daily limit met')\n",
    "            break\n",
    "        responses_done.extend(responses)\n",
    "        print(len(responses_done))\n",
    "\n",
    "    # RPM go around\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db4def42-d247-4e73-a196-ce140b40f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving progress until next rate limit reset\n",
    "sep = '\\u241E'\n",
    "with open('responses.csv', 'w', encoding='utf-8') as f:\n",
    "    if len(responses_done) > 798:\n",
    "        responses_done = responses_done[:798]\n",
    "    f.write(sep.join(responses_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc772c71-8589-4d18-b564-9c1a2de5c0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Several features support the diagnosis of Mild Cognitive Impairment (MCI). The standard deviation of normalized intensity in the left entorhinal cortex, at 7.71, is slightly below the expected range of 7.84 to 9.23, which could indicate subtle structural changes. Additionally, the maximum normalized intensity of the right rostralmiddlefrontal region, measured at 102.0, exceeds the expected range of 93.02 to 100.74, a deviation that warrants further investigation. The patient exhibits stability in cognitive and functional abilities, as evidenced by a yearly evolution of 0.0 for both the Mini-Mental State Examination and Functional Activities Questionnaire, aligning with the expected ranges.\\n\\nHowever, several counterpoints challenge the MCI diagnosis. Baseline and current Clinical Dementia Rating sum-of-boxes scores are both 0.0, well within the expected ranges, suggesting an absence of significant functional impairment. The Logical Memory Delayed Recall total scores, both at baseline (14.0) and current assessment (17.0), fall within their respective expected ranges, indicating intact episodic memory. Furthermore, the Rey Auditory Verbal Learning Test immediate recall score of 41.0 is within the normal range of 35.71 to 57.44, reflecting adequate verbal learning and memory capabilities.\\n\\nConsidering the provided data, the diagnosis of Mild Cognitive Impairment is unlikely. While some features indicate potential structural anomalies, the cognitive assessments, particularly the Clinical Dementia Rating and memory recall tests, fall within normal ranges. The patient's stable cognitive and functional performance over the year, as reflected in the Mini-Mental State Examination and Functional Activities Questionnaire, further weakens the support for an MCI diagnosis. A comprehensive evaluation, including neurological examination and advanced imaging techniques, should be performed to investigate the structural findings.\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise example Gemini response\n",
    "responses_done[760]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b909f-9390-436f-87ab-aa4992549c74",
   "metadata": {},
   "source": [
    "## Language model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92d6fb-540e-4a96-975e-d1cb0dd0b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(f\"📦 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🤗 Transformers version: {transformers.__version__}\")\n",
    "print(f\"📊 TRL version: {trl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4e04f-bfb4-4b48-8835-a1ef71f44fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import torch\n",
    "\n",
    "model_id = \"LiquidAI/LFM2-1.2B\"  # 350M\n",
    "\n",
    "print(\"📚 Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(\"🧠 Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "#   attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
    ")\n",
    "\n",
    "print(\"✅ Local model loaded successfully!\")\n",
    "print(f\"🔢 Parameters: {model.num_parameters():,}\")\n",
    "print(f\"📖 Vocab size: {len(tokenizer)}\")\n",
    "print(f\"💾 Model size: ~{model.num_parameters() * 2 / 1e9:.2f} GB (bfloat16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ea914-7c89-42dc-84c0-3f7bf234c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pickle\n",
    "\n",
    "print(\"📥 Loading SFT dataset...\")\n",
    "\n",
    "sep = '\\u241E'\n",
    "with open(\"messages.pkl\", \"rb\") as f:\n",
    "    messages = pickle.load(f)\n",
    "    \n",
    "with open('responses.csv', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    responses_done = content.split(sep)\n",
    "\n",
    "data = messages[:len(responses_done)]\n",
    "split = int(len(data)*0.9)\n",
    "for i in range(len(responses_done)):\n",
    "    data[i] = {'messages':data[i]}\n",
    "    data[i]['messages'].append({\n",
    "        'role':'assistant',\n",
    "        'content': responses_done[i]\n",
    "    })\n",
    "    \n",
    "train_dataset_sft = Dataset.from_list(data[:split])\n",
    "eval_dataset_sft = Dataset.from_list(data[split:])\n",
    "\n",
    "print(\"✅ SFT Dataset loaded:\")\n",
    "print(f\"   📚 Train samples: {len(train_dataset_sft)}\")\n",
    "print(f\"   🧪 Eval samples: {len(eval_dataset_sft)}\")\n",
    "print(f\"\\n📝 Single Sample: {train_dataset_sft[0]['messages']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556e37e-e90e-4a74-a75e-1bc80996b759",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c63684-45cc-4bc2-a493-1f91e1441452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"lfm2-sft-1B\",  # 350M\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=100,\n",
    "    warmup_ratio=0.2,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    bf16=False # <- not all colab GPUs support bf16\n",
    ")\n",
    "\n",
    "print(\"🏗️  Creating SFT trainer...\")\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset_sft,\n",
    "    eval_dataset=eval_dataset_sft,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\n🚀 Starting SFT training...\")\n",
    "sft_trainer.train()\n",
    "\n",
    "print(\"🎉 SFT training completed!\")\n",
    "\n",
    "sft_trainer.save_model()\n",
    "print(f\"💾 SFT model saved to: {sft_config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd26f9a-6b72-45e1-8d79-8f65107f9606",
   "metadata": {},
   "source": [
    "## Fine-tuned model load and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63675b6-8133-4c3f-ba72-d208fb4834c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import torch\n",
    "\n",
    "model_id = \"lfm2-sft-1B\"  # 350M\n",
    "\n",
    "print(\"📚 Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(\"🧠 Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "#   attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
    ")\n",
    "\n",
    "print(\"✅ Local model loaded successfully!\")\n",
    "print(f\"🔢 Parameters: {model.num_parameters():,}\")\n",
    "print(f\"📖 Vocab size: {len(tokenizer)}\")\n",
    "print(f\"💾 Model size: ~{model.num_parameters() * 2 / 1e9:.2f} GB (bfloat16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f3e1c-e570-4b84-ba9e-883f672e68ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def fuzzy_in_paragraph(query, paragraph, score_cutoff=80):\n",
    "    score = fuzz.partial_ratio(query, paragraph)\n",
    "    return score >= score_cutoff\n",
    "\n",
    "def parse_feature_list(feature_str):\n",
    "    \"\"\"\n",
    "    Converts a string like:\n",
    "    ['{name: Baseline Logical Memory..., value: 12.0, expected range: [10.05, 16.71]}', '{...}']\n",
    "    into a list of Python dicts.\n",
    "\n",
    "    Generated with ChatGPT\n",
    "    \"\"\"\n",
    "    # Step 1: Extract the inner `{...}` chunks\n",
    "    dict_strings = re.findall(r\"\\{.*?\\}\", feature_str)\n",
    "\n",
    "    parsed_dicts = []\n",
    "    for ds in dict_strings:\n",
    "        # Add quotes around keys\n",
    "        s = re.sub(r\"([{,]\\s*)([A-Za-z0-9 _\\-]+)(\\s*:)\", r'\\1\"\\2\"\\3', ds)\n",
    "\n",
    "        # Add quotes around string values that aren't numbers, brackets, or minus signs\n",
    "        s = re.sub(r':\\s*([A-Za-z][^,}\\]]*)', lambda m: ': \"{}\"'.format(m.group(1).strip()), s)\n",
    "\n",
    "        # Convert to Python dict via json.loads\n",
    "        parsed_dicts.append(json.loads(s))\n",
    "\n",
    "    return parsed_dicts\n",
    "  \n",
    "def compute_f1(text_names, expected_names):\n",
    "    if len(text_names) > 0:\n",
    "        tp = sum([1 for name in text_names if name in expected_names]) / len(text_names)\n",
    "        fp = sum([1 for name in text_names if name not in expected_names]) / len(text_names)\n",
    "    else: \n",
    "        tp = 0\n",
    "        fp = 0\n",
    "\n",
    "    if len(expected_names) > 0:\n",
    "        fn = sum([1 for name in expected_names if name not in text_names]) / len(expected_names)\n",
    "    else:\n",
    "        fn = 0\n",
    "    \n",
    "    if tp + fn + fn > 0:\n",
    "        f1 = (2 * tp) / (2 * tp + fp + fn)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    return f1\n",
    "\n",
    "def extract_f1_scores(text, input):\n",
    "    # Extract features from user input\n",
    "    support_input = input.split('\\nSupport:')[1].split('\\nCounterpoints')[0]\n",
    "    counter_input = input.split('\\nCounterpoints:')[1].split('\\nPrediction')[0]\n",
    "    if support_input == \"No significative supporting data\":\n",
    "        support_names = []\n",
    "    else:\n",
    "        support_input = parse_feature_list(support_input)\n",
    "        support_names = [d['name'].lower() for d in support_input]\n",
    "\n",
    "    if counter_input == \"No significative counterpoints\":\n",
    "        counter_names = []\n",
    "    else:\n",
    "        counter_input = parse_feature_list(counter_input)\n",
    "        counter_names = [d['name'].lower() for d in counter_input]\n",
    "\n",
    "    # Separate support and counterpoints sections\n",
    "    if len(text.split('\\n\\n')) == 3:\n",
    "        # Without titles\n",
    "        explanation_support = text.split('\\n\\n')[0]\n",
    "        explanation_counter = text.split('\\n\\n')[1]\n",
    "    elif len(text.split('\\n\\n')) == 6:\n",
    "        # With titles\n",
    "        explanation_support = text.split('\\n\\n')[1]\n",
    "        explanation_counter = text.split('\\n\\n')[3]\n",
    "    else:\n",
    "        # With bulletpoints\n",
    "        explanation_support = text.split('Support')[1].split('Counter')[0]\n",
    "        explanation_counter = text.split('Counter')[1].split('Predict')[0]\n",
    "\n",
    "    # Detect features from the name list\n",
    "    names = [*support_names, *counter_names]\n",
    "    text_sup_names = [name.lower() for name in names if fuzzy_in_paragraph(name.lower(), explanation_support.lower())]\n",
    "    text_cou_names = [name.lower() for name in names if fuzzy_in_paragraph(name.lower(), explanation_counter.lower())]\n",
    "    \n",
    "    # Compute confusion matrix components for Support\n",
    "    sup_f1 = compute_f1(text_sup_names, support_names)\n",
    "\n",
    "    # Compute confusion matrix components for Counterpoints\n",
    "    cou_f1 = compute_f1(text_cou_names, counter_names)\n",
    "\n",
    "    # Compute confusion matrix components for Support\n",
    "    swap1_f1 = compute_f1(text_sup_names, counter_names)\n",
    "\n",
    "    # Compute confusion matrix components for Counterpoints\n",
    "    swap2_f1 = compute_f1(text_cou_names, support_names)\n",
    "\n",
    "    return sup_f1, cou_f1, swap1_f1, swap2_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be8e1502-d49d-4363-8675-f79a515cc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = list(set([*expl_cn['name'], *expl_mci['name'], *expl_ad['name']]))\n",
    "sep = '\\u241E'\n",
    "with open('name_list.csv', 'w', encoding='utf-8') as f:\n",
    "    f.write(sep.join(name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0bcfdf-3439-4ec8-8daa-c1ea3bddba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation for all samples (except those used for fine-tuning)\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sup_f1s, cou_f1s, swap1_f1s, swap2_f1s = list(), list(), list(), list()\n",
    "i = 0\n",
    "for msg in tqdm(messages[len(responses_done):]):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        msg,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "    ).to(model.device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        min_p=0.15,\n",
    "        repetition_penalty=1.2,\n",
    "        max_new_tokens=1024,\n",
    "    )\n",
    "\n",
    "    # Get explanation text\n",
    "    explanation_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    explanation_text = explanation_text.split('assistant\\n')[1]\n",
    "\n",
    "    # Extract expected features from input\n",
    "    feature_input = msg[1]['content']\n",
    "\n",
    "    # Compute f1 scores for appearing features\n",
    "    sup_f1, cou_f1, swap1_f1, swap2_f1 = extract_f1_scores(explanation_text, feature_input)\n",
    "    if sup_f1 is not None:\n",
    "        sup_f1s.append(sup_f1)\n",
    "        cou_f1s.append(cou_f1)\n",
    "        swap1_f1s.append(swap1_f1)\n",
    "        swap2_f1s.append(swap2_f1)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"\\n\\n###\", i)\n",
    "        print(f\"F1 score for feature appearance in supporting paragraph: {np.mean(sup_f1s)} ± {np.std(sup_f1s)}\")\n",
    "        print(f\"F1 score for feature appearance in counterpoints paragraph: {np.mean(cou_f1s)} ± {np.std(cou_f1s)}\")\n",
    "        print(f\"Combined F1 score: {np.mean([*sup_f1s, *cou_f1s])} ± {np.std([*sup_f1s, *cou_f1s])}\\n\")\n",
    "\n",
    "        print(f\"F1 score for counfused feature appearance in supporting paragraph: {np.mean(swap1_f1s)} ± {np.std(swap1_f1s)}\")\n",
    "        print(f\"F1 score for counfused feature appearance in counterpoints paragraph: {np.mean(swap2_f1s)} ± {np.std(swap2_f1s)}\")\n",
    "        print(f\"Combined F1 score: {np.mean([*swap1_f1s, *swap2_f1s])} ± {np.std([*swap1_f1s, *swap2_f1s])}\")\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"### Final\")\n",
    "print(f\"F1 score for feature appearance in supporting paragraph: {np.mean(sup_f1s)} ± {np.std(sup_f1s)}\")\n",
    "print(f\"F1 score for feature appearance in counterpoints paragraph: {np.mean(cou_f1s)} ± {np.std(cou_f1s)}\")\n",
    "print(f\"Combined F1 score: {np.mean([*sup_f1s, *cou_f1s])} ± {np.std([*sup_f1s, *cou_f1s])}\\n\")\n",
    "\n",
    "print(f\"F1 score for counfused feature appearance in supporting paragraph: {np.mean(swap1_f1s)} ± {np.std(swap1_f1s)}\")\n",
    "print(f\"F1 score for counfused feature appearance in counterpoints paragraph: {np.mean(swap2_f1s)} ± {np.std(swap2_f1s)}\")\n",
    "print(f\"Combined F1 score: {np.mean([*swap1_f1s, *swap2_f1s])} ± {np.std([*swap1_f1s, *swap2_f1s])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52457da1-cbaf-49a9-8cd5-563e3e2e2b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
