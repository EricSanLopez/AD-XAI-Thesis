{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c2fb22-0e16-42f9-9d8a-d7623e374a82",
   "metadata": {},
   "source": [
    "# Language model: Supervised Fine-Tune (STF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d2de0-4a2b-44b4-894d-5f51c47675c5",
   "metadata": {},
   "source": [
    "Code adapted from LiquidAI's Supervised Fine-Tuning (SFT) notebook with a LoRA adapter in TRL (https://colab.research.google.com/drive/1j5Hk_SyBb2soUsuhU0eIEA9GwLNRnElF?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603f019-a5c8-4d94-a739-148451228a9f",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6277ce-2511-4367-befc-14ee98892d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers.git trl>=0.18.2 peft>=0.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27796364-509e-42da-bd43-11add7007a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f6356-f83c-43db-b617-6bf5775aef4f",
   "metadata": {},
   "source": [
    "## Fine-tuning dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fad89-f850-4962-bfb3-f696fcc4cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a medical expert. The user will provide a\n",
    " dataset in JSON format containing a collection of\n",
    " supporting features and counterpoint features. For each feature, there is\n",
    " the value, the percentile and the importance. The text must be based only\n",
    " on the provided data.\n",
    "\n",
    " The implications of the features on the diagnosis: if present in the\n",
    " Support section, then they support the diagnosis. If present in the\n",
    " Counterpoints section, then they do not support the diagnosis.\n",
    " \"\"\".replace(\"\\n \", \" \")\n",
    "\n",
    "def get_user_prompt(explanations_df, prediction, model):\n",
    "    # The task and output format definition is the same for all user prompts\n",
    "    start = \"\"\"Summarise the patient data into 3 cohesive and\n",
    "     medical term-rich paragraphs of 100 words each. First, the diagnosis\n",
    "     support (use only features from the Support section). Second, the\n",
    "     diagnosis counterpoints (use only features from the Counterpoints\n",
    "     section). Third, summarise and categorise the given diagnosis into possible\n",
    "     or impossible based on the data.\n",
    "    \"\"\".replace(\"\\n     \", \" \")\n",
    "\n",
    "    # Gather patient data \n",
    "    df = explanations_df[['name', 'value', 'percentile', f'importance-{model}']]\n",
    "    df = df.rename(columns={f'importance-{model}': 'importance'})\n",
    "    df['value'] = [float(f'{val:.3f}') for val in df['value']]\n",
    "    df['percentile'] = [int(per) for per in df['percentile']]\n",
    "    df['importance'] = [float(f'{imp:.3f}') for imp in df['importance']]\n",
    "    \n",
    "    # Support data\n",
    "    df_p = df[df['importance'] > 0.0]\n",
    "    p_prompt = df_p.sort_values(by='importance', ascending=False)[:10].T.to_dict()\n",
    "    p_prompt = [val for val in p_prompt.values()]\n",
    "    \n",
    "    # Counterpoint data\n",
    "    df_n = df[df['importance'] < 0.0]\n",
    "    df_n = df_n.sort_values(by='importance', ascending=True)[:10]\n",
    "    df_n['importance'] = [abs(imp) for imp in df_n['importance']]\n",
    "    n_prompt = df_n.T.to_dict()\n",
    "    n_prompt = [val for val in n_prompt.values()]\n",
    "    \n",
    "    # Join data\n",
    "    prompt = f'{start}\\n\\nPrediction: {prediction}\\nSupport: {p_prompt}\\nCounterpoints: {n_prompt}'\n",
    "    return prompt\n",
    "\n",
    "# Construct final message\n",
    "def get_messages(explanations_df, prediction, model):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": get_user_prompt(explanations_df, prediction, model)}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae4ef2-6e88-4c50-91c9-1f3efabdbd83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load explanations from the model\n",
    "patients_id = os.listdir('results')\n",
    "model_name = [\"xgboost\", \"rf\"]\n",
    "cat_map = {0: 'Cognitive Normal', 1: 'Mild Cognitive Impairment', 2: \"Alzheimer's Disease\"} \n",
    "\n",
    "messages = list()\n",
    "for ptid in patients_id:\n",
    "    path = f\"results/{ptid}/\"\n",
    "    expl_cn = pd.read_csv(path + f\"explanations-cn.csv\")\n",
    "    expl_mci = pd.read_csv(path + f\"explanations-mci.csv\")\n",
    "    expl_ad = pd.read_csv(path + f\"explanations-ad.csv\")\n",
    "    expls = [expl_cn, expl_mci, expl_ad]\n",
    "    probs = dict()\n",
    "    for model in model_name:\n",
    "        for i, cat in cat_map.items():\n",
    "            messages.append(get_messages(expls[i], cat, model))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee3460-fb42-47b6-bc07-758908f5f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('messages.pkl', 'wb') as f:\n",
    "    pickle.dump(messages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece388da-19d5-4878-8da2-6093ee760dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gemini model\n",
    "GEMINI_API_KEY = \" \"\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9307587-6190-4a67-927d-61cce99bb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read progress to continue with the generation\n",
    "sep = '\\u241E'  # Special character to separate lines safely\n",
    "try:\n",
    "    with open('responses.csv', 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        responses_done = content.split(sep)\n",
    "except FileNotFoundError:\n",
    "    responses_done = list()\n",
    "print(len(responses_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df8590-281a-47ac-8f9c-ef4ab5c72b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses (adjusted for rate limits)\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    current = len(responses_done)\n",
    "    try:\n",
    "        responses = list()\n",
    "        \n",
    "        for i, msg in enumerate(messages[current:]):\n",
    "            sys_prompt = msg[0]['content']\n",
    "            user_prompt = msg[1]['content']\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\", \n",
    "                config=types.GenerateContentConfig(\n",
    "                    system_instruction=sys_prompt),\n",
    "                contents=user_prompt,\n",
    "            )\n",
    "            responses.append(response.text)\n",
    "    except:\n",
    "        if len(responses) == 0:\n",
    "            print('Daily limit met')\n",
    "            break\n",
    "        responses_done.extend(responses)\n",
    "        print(len(responses_done))\n",
    "\n",
    "    # RPM go around\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4def42-d247-4e73-a196-ce140b40f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving progress until next rate limit reset\n",
    "sep = '\\u241E'\n",
    "with open('responses.csv', 'w', encoding='utf-8') as f:\n",
    "    f.write(sep.join(responses_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc772c71-8589-4d18-b564-9c1a2de5c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_done[760]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b909f-9390-436f-87ab-aa4992549c74",
   "metadata": {},
   "source": [
    "## Language model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92d6fb-540e-4a96-975e-d1cb0dd0b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(f\"📦 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🤗 Transformers version: {transformers.__version__}\")\n",
    "print(f\"📊 TRL version: {trl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4e04f-bfb4-4b48-8835-a1ef71f44fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import torch\n",
    "\n",
    "model_id = \"LiquidAI/LFM2-1.2B\"\n",
    "\n",
    "print(\"📚 Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(\"🧠 Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "#   attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
    ")\n",
    "\n",
    "print(\"✅ Local model loaded successfully!\")\n",
    "print(f\"🔢 Parameters: {model.num_parameters():,}\")\n",
    "print(f\"📖 Vocab size: {len(tokenizer)}\")\n",
    "print(f\"💾 Model size: ~{model.num_parameters() * 2 / 1e9:.2f} GB (bfloat16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ea914-7c89-42dc-84c0-3f7bf234c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pickle\n",
    "\n",
    "print(\"📥 Loading SFT dataset...\")\n",
    "\n",
    "sep = '\\u241E'\n",
    "with open(\"messages.pkl\", \"rb\") as f:\n",
    "    messages = pickle.load(f)\n",
    "    \n",
    "with open('responses.csv', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    responses_done = content.split(sep)\n",
    "    \n",
    "split = int(len(messages)*0.9)\n",
    "data = messages\n",
    "for i in range(len(data)):\n",
    "    data[i] = {'messages':data[i]}\n",
    "    data[i]['messages'].append({\n",
    "        'role':'assistant',\n",
    "        'content': responses_done[i]\n",
    "    })\n",
    "    \n",
    "train_dataset_sft = Dataset.from_list(data[:split])\n",
    "eval_dataset_sft = Dataset.from_list(data[split:])\n",
    "\n",
    "print(\"✅ SFT Dataset loaded:\")\n",
    "print(f\"   📚 Train samples: {len(train_dataset_sft)}\")\n",
    "print(f\"   🧪 Eval samples: {len(eval_dataset_sft)}\")\n",
    "print(f\"\\n📝 Single Sample: {train_dataset_sft[0]['messages']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556e37e-e90e-4a74-a75e-1bc80996b759",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c63684-45cc-4bc2-a493-1f91e1441452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"lfm2-sft\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=100,\n",
    "    warmup_ratio=0.2,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    bf16=False # <- not all colab GPUs support bf16\n",
    ")\n",
    "\n",
    "print(\"🏗️  Creating SFT trainer...\")\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset_sft,\n",
    "    eval_dataset=eval_dataset_sft,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\n🚀 Starting SFT training...\")\n",
    "sft_trainer.train()\n",
    "\n",
    "print(\"🎉 SFT training completed!\")\n",
    "\n",
    "sft_trainer.save_model()\n",
    "print(f\"💾 SFT model saved to: {sft_config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd26f9a-6b72-45e1-8d79-8f65107f9606",
   "metadata": {},
   "source": [
    "## Fine-tuned model load and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63675b6-8133-4c3f-ba72-d208fb4834c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import torch\n",
    "\n",
    "model_id = \"lfm2-sft\"\n",
    "\n",
    "print(\"📚 Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(\"🧠 Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "#   attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
    ")\n",
    "\n",
    "print(\"✅ Local model loaded successfully!\")\n",
    "print(f\"🔢 Parameters: {model.num_parameters():,}\")\n",
    "print(f\"📖 Vocab size: {len(tokenizer)}\")\n",
    "print(f\"💾 Model size: ~{model.num_parameters() * 2 / 1e9:.2f} GB (bfloat16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0bcfdf-3439-4ec8-8daa-c1ea3bddba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "msg = messages[2][:2]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    msg,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=True,\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    min_p=0.15,\n",
    "    repetition_penalty=1.05,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "explanation_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "explanation_text = explanation_text.split('assistant\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f395993-1617-4978-8775-84e8ce8c5c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target\\n\", messages[2][2]['content'], \"\\nGenerated\\n\", explanation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52457da1-cbaf-49a9-8cd5-563e3e2e2b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
